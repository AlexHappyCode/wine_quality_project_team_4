# -*- coding: utf-8 -*-
"""Team-4-wine-quality.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J4No44Fdl0vMtBsoQweDyS-LRuHEG5rH

# Wine Quality
### Team 4

https://archive.ics.uci.edu/ml/datasets/Wine+Quality
"""

import os
from IPython import display
import pandas as pd
import numpy as np
from numpy import arangeG
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_recall_fscore_support

read_df = pd.read_csv('https://raw.githubusercontent.com/AlexHappyCode/wine_quality_project_team_4/main/wine_quality_dataset/winequality-red.csv', ';')
white_df = pd.read_csv('https://raw.githubusercontent.com/AlexHappyCode/wine_quality_project_team_4/main/wine_quality_dataset/winequality-white.csv', ';')
white_df

# Limit read wine to [3, 4, 5, 6, 7, 8]
len(read_df[read_df.quality == 9])

# White wine [3 - 9]
len(white_df[white_df.quality == 10])

y = read_df.iloc[:, 11].values # y is a numpy array
x = read_df.drop(['quality'], axis=1) # dropping the label

"""# Split the data into trainning set and testing set

"""

x_train, x_test, y_train, y_test = train_test_split(
    x,
    y,
    test_size=0.25,
    #random_state=0
    shuffle=False
)

model = RandomForestClassifier (
    n_estimators=500,
    random_state=42,
    min_samples_split=2,
    min_samples_leaf=1,
    max_depth=10,
    bootstrap=True,
    max_samples=100
)

model.fit(x_train, y_train)

predict = model.predict(x_test)

ac = accuracy_score(predict, y_test)
print('Accuracy is:', ac)

"""# Hyperparameter tuning

The professor said we only need to tune, 2 hyperparameters. I believe that is the number of trees, and the depth of the trees.

"""

grid_ranges = {
    'n_estimators': [1000],
    'max_depth': np.arange(1, 15, 1),
    'bootstrap': [True],
    'max_samples': [100]
    #'max_depth': [100] # I think this is needed for bootstrap
}
gscv = GridSearchCV(
    estimator=model,
    param_grid=grid_ranges,
    cv=3,
    n_jobs=-1, # how many jobs to run in parallel
    verbose=1,
)
gscv_fit = gscv.fit(x_train, y_train)
best_parameters = gscv_fit.best_params_
print(best_parameters)

"""# Now we can retrain the model with our new hyperparameters"""

model = RandomForestClassifier (
    n_estimators=best_parameters['n_estimators'],
    max_depth=best_parameters['max_depth'],
    random_state=42,
    bootstrap=True,
    max_samples=100
)


model.fit(x_train, y_train)

"""# Now that we have created our new run time engine with the best hyperparameters, we can test the model"""

predict = model.predict(x_test)

ac = accuracy_score(predict, y_test)
print('Accuracy is:', ac)

"""### Calculating Metrics"""

scores = precision_recall_fscore_support(predict, y_test)
scores